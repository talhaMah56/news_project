# -*- coding: utf-8 -*-
"""berprophet_instructions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zzM7LAXkroytHXNlcSDQXD3fwhkm9als

<a href="https://colab.research.google.com/github/andrewkallai/MLTSA25_AKallai/blob/main/HW3/berprophet_instructions.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

This homework will guide you through a complete additive model analysis of a time series and familiarize you with the (fairly new) [Facebook Prophet](https://facebook.github.io/prophet/) package for time series analysis.

Prophet simplifies the implementation of analysis like "Structural Time Series modeling" and/or "Bayesian dynamic linear model": similarly to the ARMA family of models, these models attempt to recreate (and predict) a time series by linearly combining variouos components: trends, seasonalities, but also regression on exogenous variables. In this case, we will see if Uber rides area affected by weather, in additional to trends and seasonality.

Under the hood, the model will implement an optimization to find the best fit parameters for each component (mostly the relative amplitude of each component) in a Bayesian framework, either as a direct optimization which is possible since the model is linear, or by MCMC (https://github.com/facebook/prophet/issues/669)

Note that the 4 models you will create build incrementally: you are always adding one more piece of a model to the previous one (even when it is not explicitly stated that the new model contains all the components of the previous ones:
- Model 1: trend and seasonality
- Model 2: trend, seasonality, and exogenous weather variables
- Model 3: trend, seasonality, exogenous weather variables, holidays
- Model 4: trend, seasonality, exogenous weather variables, holidays, MCMC implementation (same model, different optimization)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pylab as pl
# %pylab inline

"""
# DATA ANALYSIS 0: we will use the package Prohpet, created by facebook for time series analysis.
Install the package and import it. The model that you will use is Prophet. Also import the diagnostics package from fbprophet
"""

!pip install prophet

import prophet

from prophet import Prophet
from prophet import diagnostics

"""# 1. DATA ACQUISITION 1: write a for loop that extracts files from the repository https://github.com/fivethirtyeight/uber-tlc-foil-response.

There are a number of csv files in the repository, and a zip file. Read in within the loop the 6 csv files representing uber pickup deives from April to September 2014. This can be done with a for loop iterating in a container that contains the apporopriate string identified in the file names (e.g. 'apr' for April) and composes the string using an immutable root, the month name, and the immutable ending of the file. Remember to read in the raw files. Remember that ```pd_read_csv ()``` accepts URLs, so you do not have to download the data.

At the end you need the data to be appended into a single datafraom (note: appended, i.e. concatenated, not merged.) You can achieve this in many ways, an easy (and a bit lazy one) is to append each file read in as a dataframe to a list and use ```pd.concat(list)``` to concatenate them into a single dataframe.
"""

a = []
for mon in ['apr', 'may', 'jun', 'jul', 'aug', 'sep']:
    url = f"https://raw.githubusercontent.com/fivethirtyeight/uber-tlc-foil-response/refs/heads/master/uber-trip-data/uber-raw-data-{mon}14.csv"
    a.append(pd.read_csv(url))

uberrides = pd.concat(a)

uberrides.head()

"""# DATA PREPARATION 1: convert the dates to datetime and generate a time series of rides/day.
Note that at this stage this is not the kind of time series you need. You need number of rides per unit time (e.g. per day) and we have timsetamp of each ride.
To obtain that you can use the ```groupby()``` method of your dataframe. Before that, convert the date time column to ```pd.datetime``` type (this may take a while), then you can use groupby with the "date" attribute, which means grouping by day, and use the method count of groupby to count the number of rides in each group.

Finally, rename the dataframe so that the number of rides is column ```y``` and the dates are column ```ds``` - this is required by the prophet package that you will use for the enalysis.

** plot the resulting dataframe
"""

uberrides["time"] = pd.to_datetime(uberrides["Date/Time"])
uberrides.head()
uberrides.tail()

uberrides.iloc[0]["time"].date()

uberbyday = uberrides.groupby(uberrides["time"].dt.date).count() # we did it in the bootcamp!
uberbyday.head()

uberrides["time"].dt.date

# prompt: rename the uberbyday columns so time will be ds and date/time will be y

uberbyday = uberbyday.rename(columns={'Date/Time':'y'})
uberbyday = uberbyday.rename_axis('ds').reset_index()
uberbyday.head()

uberbyday.plot(x="ds", y="y")
pl.xlabel('Date')
pl.ylabel('Number of Rides')

"""Fig 1: Time series of the daily number of Uber rides in NYC from April to September 2014. The data shows a general upward trend, with some daily fluctuations."

# DATA PREPARATION 2: stanardize the data
Subtract the mean and divide by the standard eviation the ```y``` column. Although this is not strictly necessary to work with the Prophet package, it is in general recommanded not to use large numbers in your analysis. furtehrmore, this will convert the target variable to a floating point, which is a better type for regression
"""

uberbyday["y"] = (uberbyday["y"] - uberbyday["y"].mean()) / uberbyday["y"].std()
uberbyday.plot(x="ds", y="y");
pl.xlabel('Date')
pl.ylabel('Number of Rides')
pl.title('Standardized Data')

"""Fig 2: Time series of standarized data of the daily number of Uber rides in NYC from April to September 2014. The data shows a general upward trend, with some daily fluctuations.

# DATA ANALYSIS fit the uber rides data with the prophet model.
# MODEL 1: trends and seasonality
The model will look for seasonality and trends automatically. It is a bit of a black box and I dont have a lot of experience with it, but it is really "cutting edge" in this field.
As with any object oriented package, you first need to invoke ```Phrophet()``` to create a model, call it ```simplem```, then fit it to the uber data. Forecast 1 month past the last available datapoint and print the last 5 entries of the resulting dataframe. Plot the forecast, including the uncertainty regions of the forecast and the comonents of the forcast which should be the trend and the weekly seanality. The model should naturally produce these components. If it does not look into the model and the settings by displaying the model methods (e.g. type ```siplemodel.``` and press the tab key.) Remember that ou MUST describe what you see in these plots!
"""

simplem = Prophet()
simplem.fit(uberbyday)

future = simplem.make_future_dataframe(periods=30)
forecast = simplem.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1 = simplem.plot(forecast)
pl.plot(uberbyday['ds'], uberbyday['y'], '--', label='Actual Data')
pl.legend()
pl.xlabel('Date')
pl.ylabel('Number of Rides (Standardized)')

"""Fig 3: Forecast of daily Uber rides in NYC for the next 30 days, generated using the Prophet model. The blue line represents the forecasted values, while the shaded blue region indicates the uncertainty intervals. The black dashed line shows the actual observed data. The model captures the general upward trend and incorporates weekly seasonality in the forecast."
"""

fig2 = simplem.plot_components(forecast)

"""fig 4: Components of the time series forecast generated by the Prophet model. The trend component (top) shows a steady increase in Uber rides over time. The weekly seasonality component (bottom) reveals higher ridership on weekends (Friday-Saturday) and lower ridership during the weekdays. "

# DATA ANALYSIS: model validatoin
 Crossvaliate your model and create a function to evaluate the goodness of fit of your model.


Use the chisquare per degree of freedom (also calle reduced chi square, note that this quantity has a theoretical expectation value of 1 for a perfect model): the chi square divided by the number of observations. Use 30 days for crossvalidation. The diagnostics function will return all that you need to calculate your reduce chi2. Howeve, we do not have the uncertainties to put them in the chi2 formula (we could create them from count statistics, but I wont ask you to) so assume the uncertainties are $\sigma$= 1.



NOTE: If your uncertainties are estimated correctly and you model is perfect the reduced chi2 should be rchi2=1. If you  unerestimat the uncertainties the reduced chi2 is likely to be less than 1.

You can read about cross valiation for regression here https://robjhyndman.com/hyndsight/tscv/ (but do not set the period=1 or the process would take too long)
"""

diagnostics.cross_validation?

def rchi2(y, ytrue):
  # Evaluates the goodness of fit of a Prophet model using cross-validation and reduced chi-squared.
  # ReturnsReduced chi-squared values.
    sigma = 1  # Assume uncertainties are 1
    chi2 = ((ytrue - y)**2) / (sigma**2)
    rchi2 = sum(chi2) / len(ytrue)
    return rchi2

uber_split = diagnostics.cross_validation(simplem, horizon='30 days')

rchi2(uber_split.yhat, uber_split.y)

"""# DATA ACQUISITION 2: OPTIONAL TASK download exogenous variable data.
One may guess that weather would affect the number of rides: a trivial example is that if it rains people are less likely to walk. It might not be that trivial tho, maybe if it  rains people are altogether less likely to go out.

Acquire weather data for NYC for the relevant time period from the www.worldweatheronline.com website through the wwo_hist API. You will have to 1) request a key https://www.worldweatheronline.com/developer/api/, https://www.worldweatheronline.com/developer/signup.aspx ( it is free for <500 requests).
an install and import the ```wwo_hist``` package

Save your key in a separate file and read it in. For example, you can create a file ```my_apis.csv``` in the same folder where you are working and the content of the file can be ```wwo,XXXXXXXXXXXXXXXXX```

This should download a ```New+york,ny.csv``` file which you can then read in. Convert the atetime entry to pd.datetime as you did before and merge the uber reides and the weather dataframe on the date feature. YOud rataframe should now have columns ```y, ds, tempC, precipMM```. You should stanardize the tempC and precipMM features as you did for the number of rides.

Plot each feature.
"""

#!pip install pandas==1.3.5
!pip install wwo_hist

from wwo_hist import retrieve_hist_data

from google.colab import drive

drive.mount("/content/gdrive")

cd /content/gdrive/MyDrive/

import pandas as pd
frequency = 24 #frequency in hours
start_date = '01-APR-2014'
end_date = '30-SEP-2014'
api_key = pd.read_csv("/content/gdrive/MyDrive/my_apis.csv", header=None, index_col=0).loc["wwo"].values[0]
location_list = ['New+york,ny']
hist_weather_data = retrieve_hist_data(api_key,
                                location_list,
                                start_date,
                                end_date,
                                frequency,
                                location_label = False,
                                export_csv = True)

weather = pd.read_csv("New+york,ny.csv")
weather

# weather = pd.read_csv("New+york,ny.csv")...
weather = pd.read_csv("New+york,ny.csv")
weather["date_time"] = pd.to_datetime(weather["date_time"])
weather = weather.rename(columns={"date_time": "ds"})
weather = weather[["ds", "tempC", "precipMM"]]
weather = weather.groupby("ds").mean()
weather = weather.reset_index()
# Ensure that 'ds' in uberbyday is also of datetime type
uberbyday['ds'] = pd.to_datetime(uberbyday['ds'])
uberweather = pd.merge(uberbyday, weather, on="ds", how="inner")

uberweather["tempC"] = (uberweather["tempC"] - uberweather["tempC"].mean()) / uberweather["tempC"].std()
uberweather["precipMM"] = (uberweather["precipMM"] - uberweather["precipMM"].mean()) / uberweather["precipMM"].std()

uberweather.head()

uberweather = uberbyday.merge(weather, on='ds', how='left')

#leave
uberweather

fig, ax = pl.subplots(3)
uberweather.plot(y="y", ax=ax[0])
uberweather.plot(y="tempC", ax=ax[1])
uberweather.plot(y="precipMM", ax=ax[2])

"""...caption...

# MODEL 2: create a new Prophet model and add to it tempC and precipMM as regressors
Use the model method ```model.add_regressor()```. Call the model ```regrm```. Fit the model and cross valiate it. Plot the 2 models and calculate the reduced chi2 for each model and compare. Note that here you should not forecast, since you do not have the future values of the weather. If you wanted to go through the trouble to get future weather and forecast that is definitely a good extra credit!
"""

regrm = Prophet()
regrm.add_regressor('tempC')
regrm.add_regressor('precipMM')
regrm.fit(uberweather)

nowcast = regrm.predict(uberweather)
simplem.plot(nowcast)
regrm.plot(nowcast);

"""...caption..."""

...

rchi2...

"""# MODEL 3 add holidays
Holiday of course affect traffic and ridership. Prophoet has very convenient functionality to put in holidays! This a huge simplification of the workflow in timeseries analysis! add holidays for ths us with the model ```model.add_country_holidays()``` which takes argument ```country_name="US"``` in the case of the US. Once again, fit, predict, plot, crossvalidate, calculate and compare the reduced chi2

"""

regholm ...

...

"""...caption..."""

rchi2...

"""# MODEL 4: this whole thing can be done with MCMC optimization in a fully bayesian framework.
Add the argument mcmc_samples=5000 to Propher in your next model and see how it goes. Make the usual plots and calculate the reduced chi2 one last time
"""

regmcmc = Prophet(mcmc_samples=5000)
...

rchi2...

